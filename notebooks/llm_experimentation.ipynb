{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13a2e101",
   "metadata": {},
   "source": [
    "# LLM Experimentation with LlamaIndex\n",
    "\n",
    "This notebook demonstrates the process of indexing Paul Graham's essays, querying them, and generating responses using a fine-tuned GPT-2 model. It showcases retrieval-augmented generation (RAG) techniques to enhance model responses with contextually relevant information.\n",
    "\n",
    "## Overview\n",
    "1. Load fine-tuned GPT-2 model and Paul Graham's essays index\n",
    "2. Define helper functions for context processing and response generation\n",
    "3. Query the model with retrieval augmentation\n",
    "4. Analyze and visualize results\n",
    "\n",
    "This notebook works in conjunction with the following implementation files:\n",
    "- `src/finetune_model.py`: Fine-tunes the GPT-2 model on Paul Graham's essays\n",
    "- `src/index_data.py`: Creates the vector index of essays using LlamaIndex\n",
    "- `src/generate_text.py`: Interactive interface for generating responses\n",
    "- `src/utils.py`: Utility functions for analysis and environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ecaea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import os\n",
    "import torch\n",
    "import time\n",
    "import re\n",
    "import textwrap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import LLM and index-related libraries\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from llama_index.core import VectorStoreIndex, StorageContext, load_index_from_storage, Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# Add the parent directory to path to allow importing from src\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "# Import our utility functions\n",
    "from src.utils import check_environment\n",
    "\n",
    "# Check environment and see if we have GPU support\n",
    "check_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8ffcbd",
   "metadata": {},
   "source": [
    "# Step 1: Load the Fine-Tuned Model and Index\n",
    "\n",
    "In this step, we load the fine-tuned GPT-2 model and the indexed essays. The model has been trained on Paul Graham's essays to better capture his writing style, insights, and perspectives.\n",
    "\n",
    "The vector index enables efficient semantic retrieval of relevant essay content based on user queries. This is essential for our retrieval-augmented generation approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cacc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths and configuration\n",
    "model_path = \"../models/finetuned/paul_graham_gpt2\"\n",
    "storage_path = \"../storage\"\n",
    "\n",
    "# Check if the model path exists\n",
    "if not os.path.exists(model_path):\n",
    "    print(f\"❌ Model path '{model_path}' not found. Please run finetune_model.py first.\")\n",
    "    print(f\"Example: python src/finetune_model.py --data_dir data --output_dir models/finetuned\")\n",
    "else:\n",
    "    print(f\"✓ Found fine-tuned model at {model_path}\")\n",
    "\n",
    "# Check if index exists\n",
    "if not os.path.exists(storage_path) or not os.listdir(storage_path):\n",
    "    print(f\"❌ Index not found at '{storage_path}'. Please run index_data.py first.\")\n",
    "    print(f\"Example: python src/index_data.py\")\n",
    "else:\n",
    "    print(f\"✓ Found vector index at {storage_path}\")\n",
    "\n",
    "# Set device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "try:\n",
    "    # Load tokenizer and model\n",
    "    print(\"Loading tokenizer and model...\")\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Set padding token\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path).to(device)\n",
    "    print(f\"✓ Model loaded successfully\")\n",
    "    \n",
    "    # Initialize embedding model for index\n",
    "    print(\"Initializing embedding model...\")\n",
    "    embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    \n",
    "    # Load the vector index\n",
    "    print(\"Loading vector index...\")\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=storage_path)\n",
    "    index = load_index_from_storage(storage_context, embed_model=embed_model)\n",
    "    query_engine = index.as_query_engine()\n",
    "    print(f\"✓ Index loaded successfully with {len(index.docstore.docs)} documents\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading model or index: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31d4801",
   "metadata": {},
   "source": [
    "# Step 2: Define Helper Functions\n",
    "\n",
    "We'll define several helper functions to process the context, create structured prompts for the model, and clean up the generated responses. These functions are adapted from our `generate_optimized_text.py` implementation.\n",
    "\n",
    "Key functions include:\n",
    "- `filter_context`: Extracts and organizes the most relevant portions of retrieved text\n",
    "- `create_prompt`: Constructs a well-structured prompt to guide the model's generation\n",
    "- `post_process_response`: Cleans and formats the model's output for readability\n",
    "- `generate_response`: Handles the end-to-end retrieval and generation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52d1680",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_context(context, query, max_length=700):\n",
    "    \"\"\"Filters and organizes context to make it more relevant to the query.\n",
    "    \n",
    "    Args:\n",
    "        context (str): Raw context from vector search\n",
    "        query (str): User's query\n",
    "        max_length (int): Maximum context length to return\n",
    "        \n",
    "    Returns:\n",
    "        str: Filtered, relevant context\n",
    "    \"\"\"\n",
    "    # Clean up the context\n",
    "    cleaned_context = re.sub(r'file_path:.*?\\n', '', context)\n",
    "    cleaned_context = re.sub(r'Context information is below\\.\\s*---------------------\\n', '', cleaned_context)\n",
    "    \n",
    "    # Split into paragraphs for scoring\n",
    "    paragraphs = []\n",
    "    current_paragraph = \"\"\n",
    "    \n",
    "    for line in cleaned_context.split('\\n'):\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            current_paragraph += line + \" \"\n",
    "        elif current_paragraph:  # Empty line and we have content\n",
    "            paragraphs.append(current_paragraph.strip())\n",
    "            current_paragraph = \"\"\n",
    "    \n",
    "    # Add the last paragraph if it exists\n",
    "    if current_paragraph:\n",
    "        paragraphs.append(current_paragraph.strip())\n",
    "    \n",
    "    # Extract keywords from query \n",
    "    stop_words = {'what', 'does', 'how', 'why', 'when', 'where', 'which', 'paul', 'graham', \n",
    "                  'think', 'about', 'according', 'make', 'give', 'describe', 'say', 'tell'}\n",
    "    \n",
    "    query_tokens = re.findall(r'\\b\\w+\\b', query.lower())\n",
    "    query_keywords = set([w for w in query_tokens if len(w) > 2 and w not in stop_words])\n",
    "    \n",
    "    # Score paragraphs by relevance to query\n",
    "    scored_paragraphs = []\n",
    "    for p in paragraphs:\n",
    "        p_lower = p.lower()\n",
    "        # Count keyword matches\n",
    "        keyword_matches = sum(1 for keyword in query_keywords if keyword in p_lower)\n",
    "        # Calculate keyword density\n",
    "        density = keyword_matches / (len(p) / 100) if len(p) > 0 else 0\n",
    "        # Combined score\n",
    "        score = keyword_matches * 1.5 + density * 0.5\n",
    "        scored_paragraphs.append((p, score))\n",
    "    \n",
    "    # Sort paragraphs by score (highest first)\n",
    "    sorted_paragraphs = [p for p, _ in sorted(scored_paragraphs, key=lambda x: x[1], reverse=True)]\n",
    "    \n",
    "    # Take top paragraphs up to max_length\n",
    "    filtered_context = \"\"\n",
    "    current_length = 0\n",
    "    \n",
    "    for p in sorted_paragraphs:\n",
    "        p_length = len(p)\n",
    "        if current_length + p_length + 2 <= max_length:  # +2 for newlines\n",
    "            filtered_context += p + \"\\n\\n\"\n",
    "            current_length += p_length + 2\n",
    "        else:\n",
    "            # Include at least one paragraph even if it's long\n",
    "            if filtered_context == \"\" and p_length > max_length:\n",
    "                filtered_context = p[:max_length-3] + \"...\"\n",
    "            break\n",
    "    \n",
    "    return filtered_context.strip()\n",
    "\n",
    "def create_prompt(query, context):\n",
    "    \"\"\"Creates a structured prompt for the model.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User's question\n",
    "        context (str): Relevant context for answering\n",
    "        \n",
    "    Returns:\n",
    "        str: Formatted prompt for the model\n",
    "    \"\"\"\n",
    "    # Create a well-structured prompt that guides the model\n",
    "    prompt = f\"\"\"Answer the following question about Paul Graham's essays using ONLY the information provided below.\n",
    "\n",
    "CONTEXT FROM PAUL GRAHAM'S ESSAYS:\n",
    "{context}\n",
    "\n",
    "QUESTION: {query}\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "    \n",
    "    return prompt.strip()\n",
    "\n",
    "def post_process_response(full_response, query):\n",
    "    \"\"\"Cleans and formats the model's generated response.\n",
    "    \n",
    "    Args:\n",
    "        full_response (str): Raw text from the model\n",
    "        query (str): Original query\n",
    "        \n",
    "    Returns:\n",
    "        str: Cleaned, formatted response\n",
    "    \"\"\"\n",
    "    # Extract just the answer portion\n",
    "    match = re.search(r'ANSWER:(.*)', full_response, re.DOTALL)\n",
    "    if match:\n",
    "        answer = match.group(1).strip()\n",
    "    else:\n",
    "        # If format not followed, take text after query\n",
    "        try:\n",
    "            query_index = full_response.lower().index(query.lower())\n",
    "            answer = full_response[query_index + len(query):].strip()\n",
    "        except ValueError:\n",
    "            answer = full_response\n",
    "    \n",
    "    # Clean up formatting\n",
    "    answer = re.sub(r'\\[\\d+\\]', '', answer)  # Remove citation markers\n",
    "    answer = re.sub(r'\\n{2,}', '\\n\\n', answer)  # Normalize newlines\n",
    "    answer = re.sub(r'\\s{2,}', ' ', answer)  # Normalize spaces\n",
    "    \n",
    "    # Fix incomplete sentences at the end\n",
    "    sentences = answer.split('.')\n",
    "    if len(sentences) > 1 and len(sentences[-1].strip()) < 10:\n",
    "        answer = '.'.join(sentences[:-1]) + '.'\n",
    "        \n",
    "    return answer\n",
    "\n",
    "def generate_response(query, query_engine, tokenizer, model, device, \n",
    "                      max_context_length=700, max_tokens=150,\n",
    "                      temperature=0.7, top_p=0.9):\n",
    "    \"\"\"End-to-end response generation using retrieval and LLM.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User question\n",
    "        query_engine: LlamaIndex query engine\n",
    "        tokenizer: GPT-2 tokenizer\n",
    "        model: GPT-2 model\n",
    "        device: Computation device (CPU/GPU)\n",
    "        max_context_length (int): Maximum context length\n",
    "        max_tokens (int): Maximum tokens to generate\n",
    "        temperature (float): Generation temperature\n",
    "        top_p (float): Nucleus sampling parameter\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (final_response, raw_context, filtered_context, generation_time)\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Get context from index\n",
    "    raw_response = query_engine.query(query)\n",
    "    raw_context = str(raw_response)\n",
    "    \n",
    "    # Filter to get most relevant context\n",
    "    filtered_context = filter_context(raw_context, query, max_length=max_context_length)\n",
    "    \n",
    "    # Create prompt with context\n",
    "    prompt = create_prompt(query, filtered_context)\n",
    "    \n",
    "    # Generate response with model\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            num_beams=3,\n",
    "            no_repeat_ngram_size=3,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode and post-process\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    final_response = post_process_response(generated_text, query)\n",
    "    \n",
    "    generation_time = time.time() - start_time\n",
    "    \n",
    "    return final_response, raw_context, filtered_context, prompt, generation_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2805c415",
   "metadata": {},
   "source": [
    "# Step 3: Query the Model\n",
    "\n",
    "Now let's test the model with a sample query about Paul Graham's essays. We'll demonstrate how the retrieval-augmented generation process works:\n",
    "\n",
    "1. User submits a question about Paul Graham's essays\n",
    "2. The system retrieves relevant context from the indexed essays\n",
    "3. The context is filtered and formatted into a prompt\n",
    "4. The fine-tuned model generates a response based on the prompt\n",
    "5. The response is post-processed for clarity and readability\n",
    "\n",
    "This process combines the strengths of retrieval systems (accurate information lookup) with generative AI (natural language generation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd00f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test with a sample query\n",
    "query = \"What is Paul Graham's advice on startups?\"\n",
    "\n",
    "try:\n",
    "    # Generate response\n",
    "    response, raw_context, filtered_context, prompt, gen_time = generate_response(\n",
    "        query=query,\n",
    "        query_engine=query_engine,\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "        device=device,\n",
    "        max_context_length=700,\n",
    "        max_tokens=150,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9\n",
    "    )\n",
    "    \n",
    "    # Display results\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"QUERY: {query}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(\"FILTERED CONTEXT:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(filtered_context[:300] + \"...\" if len(filtered_context) > 300 else filtered_context)\n",
    "    print(\"-\" * 80)\n",
    "    print(\"RESPONSE:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(textwrap.fill(response, width=80))\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Generation time: {gen_time:.2f} seconds\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error generating response: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791e3bd6",
   "metadata": {},
   "source": [
    "# Step 4: Experiment with Different Queries\n",
    "\n",
    "Let's try a few different queries to see how the model handles various questions about Paul Graham's essays. We'll create a simple function to test multiple queries and display the results in a consistent format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d72a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_queries(queries, display_context=False):\n",
    "    \"\"\"Test multiple queries and display results.\n",
    "    \n",
    "    Args:\n",
    "        queries (list): List of query strings\n",
    "        display_context (bool): Whether to display retrieved context\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for query in queries:\n",
    "        print(f\"Processing: '{query}'\")\n",
    "        try:\n",
    "            # Generate response\n",
    "            response, raw_context, filtered_context, prompt, gen_time = generate_response(\n",
    "                query=query,\n",
    "                query_engine=query_engine,\n",
    "                tokenizer=tokenizer,\n",
    "                model=model,\n",
    "                device=device\n",
    "            )\n",
    "            \n",
    "            results.append({\n",
    "                'query': query,\n",
    "                'response': response,\n",
    "                'context': filtered_context,\n",
    "                'time': gen_time\n",
    "            })\n",
    "            \n",
    "            # Display results\n",
    "            print(f\"Response generated in {gen_time:.2f} seconds\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "    \n",
    "    # Display all results in a nice format\n",
    "    for i, result in enumerate(results):\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"QUERY {i+1}: {result['query']}\")\n",
    "        print(\"-\" * 80)\n",
    "        print(\"RESPONSE:\")\n",
    "        print(textwrap.fill(result['response'], width=80))\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"Generation time: {result['time']:.2f} seconds\")\n",
    "        \n",
    "        if display_context:\n",
    "            print(\"\\nCONTEXT EXTRACT:\")\n",
    "            print(result['context'][:200] + \"...\")\n",
    "            \n",
    "        print(\"=\" * 80)\n",
    "        print()\n",
    "    \n",
    "    # Return all results for further analysis\n",
    "    return results\n",
    "\n",
    "# Define a set of interesting queries\n",
    "test_query_set = [\n",
    "    \"What does Paul Graham think about programming languages?\",\n",
    "    \"How does Paul Graham describe the ideal founder?\",\n",
    "    \"What is Paul Graham's philosophy on innovation?\",\n",
    "    \"What advice does Paul Graham give to young people about careers?\"\n",
    "]\n",
    "\n",
    "# Run the test queries\n",
    "query_results = test_queries(test_query_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbecf750",
   "metadata": {},
   "source": [
    "# Step 5: Visualize Context Relevance and Model Performance\n",
    "\n",
    "Let's create some visualizations to better understand how the context influences our model's responses. We'll analyze:\n",
    "\n",
    "1. Context relevance scoring - how well our ranking works\n",
    "2. Response length vs. context length\n",
    "3. Generation time analysis\n",
    "4. Keyword overlap between query, context, and response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3ad7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a more advanced visualization of context relevance\n",
    "def visualize_context_relevance(context, query):\n",
    "    \"\"\"Visualizes the relevance of context segments to the query.\"\"\"\n",
    "    # Split into paragraphs\n",
    "    paragraphs = [p.strip() for p in context.split('\\n\\n') if p.strip()]\n",
    "    \n",
    "    # Extract keywords from query\n",
    "    stop_words = {'what', 'does', 'how', 'why', 'when', 'where', 'which', 'paul', 'graham'}\n",
    "    query_tokens = re.findall(r'\\b\\w+\\b', query.lower())\n",
    "    query_keywords = [w for w in query_tokens if len(w) > 2 and w not in stop_words]\n",
    "    \n",
    "    # Calculate relevance scores based on keyword presence\n",
    "    relevance_scores = []\n",
    "    paragraph_texts = []\n",
    "    \n",
    "    for i, p in enumerate(paragraphs):\n",
    "        # Limit to first 50 chars for display\n",
    "        paragraph_texts.append(f\"P{i+1}: {p[:50]}...\")\n",
    "        \n",
    "        # Score based on keyword matches\n",
    "        p_lower = p.lower()\n",
    "        keyword_matches = sum(1 for keyword in query_keywords if keyword in p_lower)\n",
    "        density = keyword_matches / (len(p) / 100) if len(p) > 0 else 0\n",
    "        score = keyword_matches * 1.5 + density * 0.5\n",
    "        relevance_scores.append(score)\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(paragraph_texts, relevance_scores, color=\"skyblue\")\n",
    "    \n",
    "    # Highlight most relevant paragraph\n",
    "    if relevance_scores:\n",
    "        max_idx = relevance_scores.index(max(relevance_scores))\n",
    "        bars[max_idx].set_color('orange')\n",
    "    \n",
    "    plt.xlabel(\"Context Paragraphs\")\n",
    "    plt.ylabel(\"Relevance Score\")\n",
    "    plt.title(f\"Context Relevance to Query: '{query}'\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "    \n",
    "    # Return the most relevant paragraph\n",
    "    if paragraphs:\n",
    "        max_idx = relevance_scores.index(max(relevance_scores))\n",
    "        return paragraphs[max_idx]\n",
    "    return \"\"\n",
    "\n",
    "# Try the visualization on our first query\n",
    "sample_query = \"What is Paul Graham's advice on startups?\"\n",
    "sample_response, sample_raw_context, sample_filtered_context, sample_prompt, _ = generate_response(\n",
    "    query=sample_query,\n",
    "    query_engine=query_engine,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "most_relevant_paragraph = visualize_context_relevance(sample_filtered_context, sample_query)\n",
    "print(\"\\nMost relevant paragraph:\")\n",
    "print(\"-\" * 40)\n",
    "print(textwrap.fill(most_relevant_paragraph, width=80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517eb883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Performance Analysis\n",
    "\n",
    "# 1. Try different temperature settings to see impact on responses\n",
    "temperature_values = [0.2, 0.5, 0.8, 1.0]\n",
    "temp_query = \"What makes a successful startup according to Paul Graham?\"\n",
    "\n",
    "print(\"Testing different temperature values...\")\n",
    "temp_results = []\n",
    "\n",
    "for temp in temperature_values:\n",
    "    response, _, _, _, gen_time = generate_response(\n",
    "        query=temp_query,\n",
    "        query_engine=query_engine,\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "        device=device,\n",
    "        temperature=temp\n",
    "    )\n",
    "    temp_results.append((temp, response, gen_time))\n",
    "    print(f\"Temperature {temp}: Generated in {gen_time:.2f}s\")\n",
    "\n",
    "# Display the results for comparison\n",
    "print(\"\\nComparing responses at different temperatures:\")\n",
    "for temp, response, _ in temp_results:\n",
    "    print(f\"\\nTemperature = {temp}:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(textwrap.fill(response[:200] + \"...\", width=80))\n",
    "    \n",
    "# Create metrics for response quality\n",
    "def get_response_metrics(response):\n",
    "    metrics = {\n",
    "        'length': len(response),\n",
    "        'sentences': len(re.split(r'[.!?]', response)),\n",
    "        'words': len(re.findall(r'\\b\\w+\\b', response))\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "# Get metrics for each temperature\n",
    "metrics_data = {'temperature': [], 'length': [], 'sentences': [], 'words': [], 'time': []}\n",
    "for temp, response, gen_time in temp_results:\n",
    "    metrics = get_response_metrics(response)\n",
    "    metrics_data['temperature'].append(temp)\n",
    "    metrics_data['length'].append(metrics['length'])\n",
    "    metrics_data['sentences'].append(metrics['sentences'])\n",
    "    metrics_data['words'].append(metrics['words'])\n",
    "    metrics_data['time'].append(gen_time)\n",
    "\n",
    "# Plot metrics vs temperature\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Response length metrics\n",
    "ax1.plot(metrics_data['temperature'], metrics_data['words'], 'o-', label='Word Count')\n",
    "ax1.plot(metrics_data['temperature'], metrics_data['sentences'], 's-', label='Sentence Count')\n",
    "ax1.set_xlabel('Temperature')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.set_title('Response Length Metrics vs Temperature')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend()\n",
    "\n",
    "# Generation time\n",
    "ax2.plot(metrics_data['temperature'], metrics_data['time'], 'o-', color='orange')\n",
    "ax2.set_xlabel('Temperature')\n",
    "ax2.set_ylabel('Generation Time (seconds)')\n",
    "ax2.set_title('Generation Time vs Temperature')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8e3c57",
   "metadata": {},
   "source": [
    "# Step 6: Evaluating Response Quality\n",
    "\n",
    "Let's analyze the quality of our responses compared to direct generation (without retrieval augmentation). This will help us understand the benefits of the retrieval-augmented generation approach.\n",
    "\n",
    "We'll compare:\n",
    "1. Response with retrieval (context-aware)\n",
    "2. Response without retrieval (model knowledge only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e876d8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_direct_response(query, tokenizer, model, device, max_tokens=150):\n",
    "    \"\"\"Generate response without retrieval augmentation.\"\"\"\n",
    "    # Create a direct prompt\n",
    "    prompt = f\"QUESTION: {query}\\n\\nANSWER:\"\n",
    "    \n",
    "    # Generate response\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode and post-process\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    final_response = post_process_response(generated_text, query)\n",
    "    \n",
    "    return final_response\n",
    "\n",
    "# Compare retrieval vs non-retrieval responses\n",
    "comparison_query = \"What does Paul Graham think about risk-taking in startups?\"\n",
    "\n",
    "print(\"Generating responses for comparison...\")\n",
    "\n",
    "# Response with retrieval\n",
    "retrieval_response, raw_context, filtered_context, _, _ = generate_response(\n",
    "    query=comparison_query,\n",
    "    query_engine=query_engine,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Response without retrieval\n",
    "direct_response = generate_direct_response(\n",
    "    query=comparison_query,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Display the comparison\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"QUERY: {comparison_query}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nRESPONSE WITH RETRIEVAL:\")\n",
    "print(\"-\" * 40)\n",
    "print(textwrap.fill(retrieval_response, width=80))\n",
    "\n",
    "print(\"\\nRESPONSE WITHOUT RETRIEVAL:\")\n",
    "print(\"-\" * 40)\n",
    "print(textwrap.fill(direct_response, width=80))\n",
    "\n",
    "print(\"\\nKEY CONTEXT USED:\")\n",
    "print(\"-\" * 40)\n",
    "context_preview = filtered_context[:300] + \"...\" if len(filtered_context) > 300 else filtered_context\n",
    "print(textwrap.fill(context_preview, width=80))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87052a94",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to use LlamaIndex and a fine-tuned GPT-2 model to create a system that can answer questions about Paul Graham's essays. Key components of our approach include:\n",
    "\n",
    "1. **Vector Indexing**: Storing and retrieving essay content using semantic search\n",
    "2. **Context Filtering**: Identifying the most relevant passages for a given query\n",
    "3. **Fine-tuned Model**: Using a model specifically trained on Paul Graham's writing style\n",
    "4. **Retrieval-Augmented Generation**: Combining retrieval and generation for more accurate responses\n",
    "\n",
    "This approach helps address some of the limitations of traditional language models, such as hallucinations and outdated knowledge, by grounding responses in specific source material.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "To further improve this system, consider:\n",
    "\n",
    "1. Experimenting with different embedding models for better retrieval\n",
    "2. Fine-tuning larger models like GPT-J or LLaMA\n",
    "3. Implementing a feedback mechanism to improve response quality over time\n",
    "4. Adding source citations to responses for better transparency"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
